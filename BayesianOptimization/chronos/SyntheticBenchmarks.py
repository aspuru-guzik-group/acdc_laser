from typing import Union, Type, Tuple, List, Optional, Callable
from pathlib import Path
import shutil
import copy
import time
import logging
import numpy as np
import torch
from botorch.test_functions import *
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from .Optimizer import DiscreteGridOptimizer
from .SurrogateModels import SurrogateModel
from .AcquisitionFunctions import DiscreteQAcquisitionFunction


class DiscreteSurface(object):

    def __init__(
            self,
            name: str,
            surface: str,
            dimensions: int,
            value_range: Tuple[float, float],
            systematic_grid: bool,
            grid_size: Optional[int] = None,
            grid_size_per_dimension: Optional[int] = None,
            noise_level: Optional[float] = None,
            random_seed: Optional[int] = 42
    ):
        """
        Instantiates a discretized version of an analytical response surface.

        Args:
            name: Name of the surface.
            surface: Name of the surface function (must be a function from botorch.test_functions).
            dimensions: Number of dimensions of the surface.
            value_range: Range of the x values per dimension (min, max).
            systematic_grid: If True, the grid points are generated by linear spacing.
            grid_size: Total number of grid points. Ignored if systematic_grid is True.
            grid_size_per_dimension: Number of grid points per dimension (alternative to grid_size). Only used if
                                     systematic_grid is True.
            noise_level: Noise level of the surface.
            random_seed: Random seed for the grid generation.
        """
        self._name: str = name
        self._surface: SyntheticTestFunction = eval(surface)(dim=dimensions, noise_std=noise_level, negate=True)
        self._dimensions = dimensions
        self._seed = random_seed
        self._grid = self._setup_grid(value_range, systematic_grid, dimensions, grid_size, grid_size_per_dimension)
        self._optimum = self._find_optimum()

    def _setup_grid(
            self,
            value_range: Tuple[float, float],
            systematic_grid: bool,
            dimensions: int,
            grid_size: Optional[int] = None,
            grid_size_per_dim: Optional[int] = None,
    ) -> np.ndarray:
        """
        Generates a grid of data points for the optimization.
            - If systematic_grid is True, the grid points are generated by linear spacing.
            - If systematic_grid is False, the grid points are generated by random sampling.

        Args:
            value_range: Range of the x values per dimension (min, max).
            systematic_grid: If True, the grid points are generated by linear spacing.
            dimensions: Number of dimensions of the surface.
            grid_size: Total number of grid points. Ignored if systematic_grid is True.
            grid_size_per_dim: Number of grid points per dimension (alternative to grid_size). Only used if
                               systematic_grid is True.

        Returns:
            np.ndarray: Grid of data points (grid_size, dimensions).
        """
        if systematic_grid is True and grid_size_per_dim is None:
            raise ValueError("If systematic_grid is True, grid_size_per_dim must be specified.")
        elif systematic_grid is False and grid_size is None:
            raise ValueError("If systematic_grid is False, grid_size must be specified.")

        np.random.seed(self._seed)

        if systematic_grid:
            single_dimension = np.linspace(value_range[0], value_range[1], grid_size_per_dim)
            grid = np.meshgrid(*[single_dimension for _ in range(dimensions)])
            grid = np.stack(grid, axis=-1)
            grid = grid.reshape(-1, dimensions)
        else:
            grid = np.random.uniform(value_range[0], value_range[1], (grid_size, dimensions))

        return grid

    def _find_optimum(self) -> float:
        """
        Finds the global optimum of the surface.

        Returns:
            float: Objective value of the global optimum.
        """
        objective_values = self._surface.evaluate_true(torch.from_numpy(self._grid)).detach().numpy().reshape(-1, 1)
        return np.max(objective_values)

    def __call__(self, x: np.ndarray) -> np.ndarray:
        """
        Evaluates the surface at the given points.

        Args:
            x: Numpy array (n_points, dimensions) of points to be evaluated.

        Returns:
            np.ndarray: Objective values at the given data points.
        """
        torch.manual_seed(self._seed)
        return self._surface(torch.from_numpy(x)).detach().numpy().reshape(-1, 1)

    @property
    def grid(self) -> np.ndarray:
        return copy.deepcopy(self._grid)

    @property
    def optimum(self) -> float:
        return self._optimum

    @property
    def grid_size(self) -> int:
        return self._grid.shape[0]

    @property
    def seed(self) -> int:
        return self._seed

    @seed.setter
    def seed(self, seed: int) -> None:
        self._seed = seed

    def plot_histogram(
            self,
            working_dir: Path
    ) -> None:
        """
        Plots a histogram of the objective values of the grid points. Saves the histogram data into a
        compressed numpy archive.

        Args:
            working_dir: Path to the directory where the histogram should be saved.
        """

        objective_values = self(self._grid)

        fig, ax = plt.subplots(1, 1, figsize=(5, 5))
        ax.set_title(f"{self._name} (Discrete, {self._dimensions}D, {self._grid.shape[0]} Points)")
        n, bins, patches = ax.hist(objective_values, bins=20)
        ax.set_xlabel("Objective Value")
        ax.set_ylabel("Frequency")
        plt.savefig(working_dir / f"{self._name}_histogram.png", dpi=600, transparent=True)
        np.savez_compressed(working_dir / f"{self._name}_histogram.npz", n=n, bins=bins)
        plt.close()


class DiscreteBOBenchmark(object):

    def __init__(
        self,
        data_dir: Path,
        surrogate_model: Type[SurrogateModel],
        surrogate_params: dict,
        acquisition_functions: List[Type[DiscreteQAcquisitionFunction]],
        acquisition_function_params: List[dict],
        acquisition_params: dict,
        n_runs: int,
        budget: int,
        batch_size: int,
        seed_size: int,
        acdc_settings: Optional[dict],
        surface: DiscreteSurface,
        objective_index: Optional[int],
        feature_scaler: type = StandardScaler,
        target_scaler: type = StandardScaler,
        random_seed: Optional[int] = 42,
        verbose: bool = False,
        **kwargs
    ):
        """

        Args:
            data_dir: Path to the directory where the data is stored.
            surrogate_model: SurrogateModel type. Must implement a train_model() and a posterior() method.
            surrogate_params: Dictionary of parameters for the surrogate model.
            acquisition_functions: List of DiscreteQAcquisitionFunction types to be used in the portfolio.
            acquisition_params: List of dictionaries of parameters for the acquisition functions.
            n_runs: Number of benchmark runs to average over.
            budget: Total number of data points to be evaluated.
            batch_size: Number of data points to be evaluated in each step.
            seed_size: Number of random data points to be evaluated in the first step.
            acdc_settings: Dictionary of settings for the ACDC algorithm.
            surface: Synthetic test function to be optimized. Callable with signature f(x: np.ndarray) -> np.ndarray.
            dimensions: Number of dimensions of the test function.
            objective_index: Index of the objective to be optimized. None for single-objective optimization.
            value_range: Range of the test function input values (Tuple[min, max]).
            grid_size: Number of grid points per dimension.
            systematic_grid: True if the grid points in each dimension should be generated by linear spacing.
            feature_scaler: Scaler type for scaling the feature values (following the scikit-learn API).
            target_scaler: Scaler type for scaling the target values (following the scikit-learn API).
            **kwargs: Additional keyword arguments to be passed to the optimizer.
        """
        self._data_dir = data_dir
        self._data_dir.mkdir(parents=True, exist_ok=True)

        self._logger = logging.getLogger(f"Run_Benchmarks_{time.time()}")
        if verbose:
            self._logger.setLevel(logging.DEBUG)
        else:
            self._logger.setLevel(logging.INFO)
        formatter = logging.Formatter("%(asctime)s - %(message)s", datefmt="%d-%b-%y %H:%M:%S")
        file_handler = logging.FileHandler(data_dir / "progress.log")
        file_handler.setFormatter(formatter)
        self._logger.addHandler(file_handler)
        if verbose:
            stream_handler = logging.StreamHandler()
            stream_handler.setFormatter(formatter)
            self._logger.addHandler(stream_handler)
        self._logger.info("Global Process Started")

        self._n_runs = n_runs
        self._budget = budget
        self._batch_size = batch_size
        self._acdc_settings = acdc_settings
        self._seed_size = seed_size

        self._optimizer_args = {
            "data_dir": data_dir,
            "surrogate_model": surrogate_model,
            "surrogate_params": surrogate_params,
            "iteration_number": 0,
            "acquisition_functions": acquisition_functions,
            "acquisition_function_params": acquisition_function_params,
            "feature_scaler": feature_scaler,
            "target_scaler": target_scaler,
            "logger": self._logger,
            **acquisition_params
        } | kwargs

        np.random.seed(random_seed)

        self._surface = surface
        self._objective_index = objective_index

    def __call__(self):
        """
        Runs a benchmark optimization on the model surface. Performs self._n_runs independent optimization runs and
        averages the results. All results are saved in a .npz archive in the data directory (details about its content
        can be found in the _process_optimization_performance() method).
        """
        # TODO: Add support for parallelization if run on CPU.
        all_runs = [self._benchmark_run(idx) for idx in range(self._n_runs)]
        self._process_optimization_performance(
            all_runs=[run[0] for run in all_runs],
            all_run_times=[run[1] for run in all_runs],
        )

    def _benchmark_run(self, index: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Performs a single optimization run on the model surface.

        Args:
            index: Index of the optimization run.

        Returns:
            np.ndarray (self._budget, ): Array of all function values evaluated during the benchmark run.
        """
        self._logger.info(f"Starting run {index}.")
        self._surface.seed = index
        data_x, data_y, grid = self._get_seed(random_seed=index)
        iteration_numbers = np.zeros(data_y.shape[0])
        pending_x = np.empty((0, data_x.shape[1]))

        # Create a temporary directory for the optimizer to store its data
        local_dir = self._data_dir / "tmp" / f"run_{index}"
        local_dir.mkdir(exist_ok=True, parents=True)

        # Check if the run has already been completed
        if (local_dir / "data.npz").exists():
            previous_run = np.load(local_dir / "data.npz", allow_pickle=True)
            if "completed" in previous_run.keys():
                self._logger.info(f"Run {index} already completed.")
                self._logger.info("")
                return previous_run["data_y"], previous_run["iteration_numbers"]
            else:
                (local_dir / "data.npz").unlink()
                self._logger.info(f"Run {index} was not completed. Restarting.")

        # Perform the actual optimization run
        self._optimizer_args["data_dir"] = local_dir

        iteration_number = 0
        while len(data_x) < self._budget:

            self._optimizer_args["iteration_number"] = iteration_number

            # Perform the actual Bayesian optimization
            optimizer = DiscreteGridOptimizer(**self._optimizer_args)
            recommended_indices, acqf_values = optimizer(
                observations_features=data_x,
                observations_targets=data_y,
                objective_index=self._objective_index,
                search_space_features=grid,
                pending_data_features=pending_x,
                batch_size=self._batch_size,
            )

            if self._acdc_settings:
                np.random.seed(int(time.time()))

                # Sample the number of observations that are directly obtained from the recommendations in this
                # iteration, the number of pending observations that are made from the recommendations in this
                # iteration, and the number of observations that are made from the pending observations from the
                # previous iterations.
                acquisition_settings = self._acdc_settings.get("acquisition", (0.5, 0.5))
                no_acquired_samples = np.clip(
                    int(np.random.normal(acquisition_settings[0] * self._batch_size, acquisition_settings[1] * self._batch_size)),
                    1,
                    self._batch_size
                ).astype(int)

                direct_observation_settings = self._acdc_settings.get("direct_observations", (0.5, 0.5))
                no_observed_samples = np.clip(
                    int(np.random.normal(direct_observation_settings[0] * no_acquired_samples, direct_observation_settings[1] * no_acquired_samples)),
                    1,
                    no_acquired_samples
                ).astype(int)

                from_queue_settings = self._acdc_settings.get("from_queue", (0.5, 0.5))
                no_samples_from_queue = np.clip(
                    int(np.random.normal(from_queue_settings[0] * pending_x.shape[0], from_queue_settings[1] * pending_x.shape[1])),
                    0,
                    pending_x.shape[0]
                ).astype(int)

                # Get the indices of the directly obtained observations and the pending observations
                acquisition_indices = np.arange(no_acquired_samples)
                np.random.shuffle(acquisition_indices)
                observation_indices = recommended_indices[acquisition_indices[:no_observed_samples]]
                pending_indices = recommended_indices[acquisition_indices[no_observed_samples:]]

                # Get all observations obtained in this iteration (directly and from pending queue), and the new set of
                # pending observations to append to the queue
                observation_x = np.concatenate((grid[observation_indices], pending_x[:no_samples_from_queue]))
                pending_x = np.concatenate((pending_x[no_samples_from_queue:], grid[pending_indices]))

                self._logger.debug(f"Generated {observation_x.shape[0]} new observations ({observation_indices.shape[0]} from this iteration).")
                self._logger.debug(f"{pending_x.shape[0]} experiments pending.")

            else:
                observation_indices = recommended_indices
                observation_x = grid[observation_indices]
                pending_indices = np.asarray([], dtype=int)
                self._logger.debug(f"Generated {observation_x.shape[0]} new observations.")

            # Update the data and the search space
            data_x = np.concatenate([data_x, observation_x])
            data_y = np.concatenate([data_y, self._surface(observation_x)])
            iteration_numbers = np.concatenate([iteration_numbers, np.full(observation_x.shape[0], iteration_number + 1)])
            grid = np.delete(grid, np.concatenate((observation_indices, pending_indices)), axis=0)

            iteration_number += 1
            np.savez_compressed(local_dir / "data.npz", data_y=data_y.flatten(), iteration_numbers=iteration_numbers.flatten())
            self._logger.info(f"Iteration {iteration_number-1} (run {index}) completed.")

        np.savez_compressed(
            local_dir / "data.npz",
            data_y=data_y.flatten(),
            iteration_numbers=iteration_numbers.flatten(),
            completed=np.asarray([True])
        )
        self._logger.info(f"Run {index} completed ({iteration_number} iterations).")
        self._logger.info("")

        return data_y, iteration_numbers

    def _get_seed(self, random_seed: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Randomly selects a subset of the grid points as seed data points.

        Args:
            random_seed (int): Random seed for the random number generator.

        Returns:
            np.ndarray: X values of the seed data points (seed_size, dimensions).
            np.ndarray: Y values of the seed data points (seed_size, 1).
            np.ndarray: Remaining grid points (grid_size ** dimensions - seed_size, dimensions).
        """
        grid = self._surface.grid
        np.random.seed(random_seed)
        seed_indices = np.random.choice(grid.shape[0], self._seed_size, replace=False)
        seed = grid[seed_indices]
        seed_values = self._surface(seed)
        remaining_grid = np.delete(grid, seed_indices, axis=0)

        return seed, seed_values, remaining_grid

    def _process_optimization_performance(self, all_runs: List[np.ndarray], all_run_times: List[np.ndarray]) -> None:
        """
        Processes the optimization performance for a set of benchmark runs.
        Computes the cumulative optimum for each run, and calculates the mean and standard deviation of the cumulative
        optimum over all runs.

        Saves a npz archive of all results of the optimization run:
            - raw_data: np.ndarray (n_runs, budget) of all function values evaluated during the benchmark run.
            - individual_runs: np.ndarray (n_runs, budget) of the cumulative optimum for each run.
            - mean: np.ndarray (budget, ) of the mean of the cumulative optimum over all runs.
            - std: np.ndarray (budget, ) of the standard deviation of the cumulative optimum over all runs.
            - frac_explored: np.ndarray (budget, ) of the fraction of the grid explored at each point.
            - optimum: float: Achievable optimum of the benchmark function on the grid.

        Args:
            all_runs: List of ndarrays (budget) of all function values evaluated during the benchmark run.
            all_run_times: List of integers (number of experimental iterations) of the cumulative run time of each run.
        """
        # fill runs with their maximum value if they are shorter than the longest run
        max_length = max([len(run) for run in all_runs])
        for idx, (run, run_times) in enumerate(zip(all_runs, all_run_times)):
            if len(run) < max_length:
                all_runs[idx] = np.concatenate((run, np.full(max_length - len(run), np.max(run)).reshape(-1, 1)))
                all_run_times[idx] = np.concatenate((run_times, np.full(max_length - len(run_times), np.max(run_times))))
        all_runs = np.stack(all_runs, axis=0)
        all_run_times = np.stack(all_run_times, axis=0).astype(int)

        # compute the cumulative optimum (per batch, given by all_run_times)) for each run, and calculates min and std
        runs_processed = copy.deepcopy(all_runs)
        for run_idx, run_data in enumerate(runs_processed):
            for iteration in range(np.max(all_run_times[run_idx, :]), 0, -1):
                indices_iteration = np.where(all_run_times[run_idx, :] == iteration)[0]
                if len(indices_iteration) > 0:
                    runs_processed[run_idx, indices_iteration] = np.max(run_data[:indices_iteration[0]])

        mean, std = np.mean(runs_processed, axis=0).flatten(), np.std(runs_processed, axis=0).flatten()

        np.savez_compressed(
            str(self._data_dir / "all_results.npz"),
            raw_data=all_runs,
            individual_runs=runs_processed,
            mean=mean,
            std=std,
            frac_explored=np.arange(0, self._budget) / self._surface.grid_size,
            all_run_times=np.asarray(all_run_times),
            optimum=np.asarray([self._surface.optimum])
        )

        # Removes the temporary directory created by the optimizer.
        shutil.rmtree(self._data_dir / "tmp")

